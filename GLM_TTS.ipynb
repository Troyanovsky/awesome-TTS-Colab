{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Troyanovsky/awesome-TTS-Colab/blob/main/GLM_TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B51gJgKjOsd"
      },
      "source": [
        "# ðŸ—£ï¸ GLM-TTS Colab\n",
        "\n",
        "## ðŸ“„ Description\n",
        "\n",
        "This notebook runs **GLM-TTS**, a **LLM-powered text-to-speech system** featuring **zero-shot voice cloning**, **RL-enhanced emotion control**, and **streaming real-time synthesis**.  \n",
        "Built with a **two-stage architecture**â€”speech token generation via an LLM and waveform synthesis via Flow Matchingâ€”GLM-TTS produces **high-fidelity, expressive**, and **bilingual (EN/ZH)** speech suitable for interactive applications.\n",
        "\n",
        "**Capabilities:**  \n",
        "Zero-Shot Voice Cloning (3â€“10s), RL-Tuned Emotion & Prosody, High-Quality Speech Generation, Phoneme-Level Control, Streaming / Low-Latency TTS, Chineseâ€“English Mixed Input\n",
        "\n",
        "---\n",
        "\n",
        "## How to use\n",
        "\n",
        "* Adjust the text/audio inputs in the provided fields  \n",
        "* Expand and run the section that you need. Note that you may need to restart the session after the `pip install` cells and then run following cells for libraries to work properly.\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ Model Highlights\n",
        "\n",
        "* ðŸ—£ **Zero-shot cloning** â€“ reproduce a speaker from a few seconds of audio  \n",
        "* ðŸŽ­ **Emotion-optimized prosody** â€“ Multi-reward RL (GRPO) improves expressiveness  \n",
        "* ðŸ”¤ **Hybrid phoneme+text input** â€“ precise control over pronunciation  \n",
        "* âš¡ **Streaming inference** â€“ supports real-time generation for interactive settings  \n",
        "* ðŸŒ **Bilingual capability** â€“ robust for Chinese, English, and mixed-language text  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Model Details\n",
        "\n",
        "* **Architecture:** LLM-based speech token generator + Flow Matching vocoder  \n",
        "* **Supported Languages:** English, Chinese, mixed text  \n",
        "* **Voice Cloning:** 3â€“10 seconds prompt audio  \n",
        "* **Control Features:** Emotion, prosody, phoneme-level instructions  \n",
        "* **Performance:** Low-latency streaming suitable for live applications  \n",
        "* **Format:** Available via Hugging Face  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”— Resources\n",
        "\n",
        "* **GitHub Repository:** https://github.com/zai-org/GLM-TTS  \n",
        "* **Model Availability:** https://huggingface.co/zai-org/GLM-TTS  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ™ï¸ Explore More TTS Models\n",
        "\n",
        "Looking for more cutting-edge voice models?  \n",
        "ðŸ‘‰ Check out the full collection: [awesome-TTS-Colab](https://github.com/Troyanovsky/awesome-TTS-Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TTS (using default example audio)"
      ],
      "metadata": {
        "id": "3JTZFlTiICCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup: Clone repo, install deps, download checkpoints\n",
        "\n",
        "!git clone https://github.com/zai-org/GLM-TTS.git\n",
        "%cd /content/GLM-TTS\n",
        "\n",
        "# Remove any line containing WeTextProcessing from requirements.txt\n",
        "import re\n",
        "\n",
        "req_path = \"requirements.txt\"\n",
        "\n",
        "with open(req_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Filter out the problematic line(s)\n",
        "cleaned = []\n",
        "for line in lines:\n",
        "    if not re.search(r'wetextprocessing', line, re.IGNORECASE):\n",
        "        cleaned.append(line)\n",
        "\n",
        "with open(req_path, \"w\") as f:\n",
        "    f.writelines(cleaned)"
      ],
      "metadata": {
        "id": "MD1HridKIls_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âš ï¸ You may need to restart session after installing these libraries, then run following cells\n",
        "!pip install -r requirements.txt\n",
        "!pip install -U \"huggingface-hub>=0.34.0,<1.0\"\n",
        "!pip install WeTextProcessing"
      ],
      "metadata": {
        "id": "quLceeE9-8wZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# Download full GLM-TTS checkpoint into ./ckpt\n",
        "snapshot_download(\n",
        "    \"zai-org/GLM-TTS\",\n",
        "    local_dir=\"ckpt\",\n",
        "    local_dir_use_symlinks=False,\n",
        ")"
      ],
      "metadata": {
        "id": "OETt096XKdXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text that GLM-TTS will synthesize\n",
        "input_text = \"Hello, this is Jiayan from GLM-TTS. How are you doing?\"  #@param {type:\"string\"}\n",
        "\n",
        "# Reference text that matches the built-in reference audio speaker\n",
        "prompt_text = \"I wonder if you'd like to have a burger with me.\"  #@param {type:\"string\"}\n",
        "\n",
        "# Built-in reference audio shipped with the repo\n",
        "prompt_wav_path = \"examples/prompt/jiayan_en.wav\"  #@param {type:\"string\"}\n",
        "\n",
        "# Inference options\n",
        "sample_rate = 24000  #@param {type:\"integer\"}\n",
        "use_phoneme = False  #@param {type:\"boolean\"}\n",
        "use_cache = True     #@param {type:\"boolean\"}\n",
        "seed = 0             #@param {type:\"integer\"}\n"
      ],
      "metadata": {
        "id": "nuiWBHFYIEg_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/GLM-TTS\n",
        "\n",
        "import torch\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# Import GLM-TTS helpers from the repo\n",
        "from glmtts_inference import load_models, generate_long, DEVICE\n",
        "\n",
        "# Load all frontends + models (LLM + Flow)\n",
        "frontend, text_frontend, speech_tokenizer, llm, flow = load_models(\n",
        "    use_phoneme=use_phoneme,\n",
        "    sample_rate=sample_rate,\n",
        ")\n",
        "\n",
        "def glmtts_synthesize(\n",
        "    prompt_wav: str,\n",
        "    prompt_text: str,\n",
        "    synth_text: str,\n",
        "    sample_rate: int = 24000,\n",
        "    use_phoneme: bool = False,\n",
        "    seed: int = 0,\n",
        "    use_cache: bool = True,\n",
        "):\n",
        "    \"\"\"Minimal wrapper around generate_long for a single utterance.\"\"\"\n",
        "    # Normalize text\n",
        "    prompt_text_norm = text_frontend.text_normalize(prompt_text)\n",
        "    synth_text_norm = text_frontend.text_normalize(synth_text)\n",
        "\n",
        "    # Extract tokens & features from prompt\n",
        "    prompt_text_token = frontend._extract_text_token(prompt_text_norm + \" \")\n",
        "    prompt_speech_token = frontend._extract_speech_token([prompt_wav])\n",
        "    speech_feat = frontend._extract_speech_feat(prompt_wav, sample_rate=sample_rate)\n",
        "    embedding = frontend._extract_spk_embedding(prompt_wav)\n",
        "\n",
        "    cache_speech_token = [prompt_speech_token.squeeze().tolist()]\n",
        "    flow_prompt_token = torch.tensor(cache_speech_token, dtype=torch.int32).to(DEVICE)\n",
        "\n",
        "    # LLM cache (for longer text, can reuse history)\n",
        "    cache = {\n",
        "        \"cache_text\": [prompt_text_norm],\n",
        "        \"cache_text_token\": [prompt_text_token],\n",
        "        \"cache_speech_token\": cache_speech_token,\n",
        "        \"use_cache\": use_cache,\n",
        "    }\n",
        "\n",
        "    uttid = \"simple_demo\"\n",
        "\n",
        "    # Core generation\n",
        "    tts_speech, _, _, _ = generate_long(\n",
        "        frontend=frontend,\n",
        "        text_frontend=text_frontend,\n",
        "        llm=llm,\n",
        "        flow=flow,\n",
        "        text_info=[uttid, synth_text_norm],\n",
        "        cache=cache,\n",
        "        embedding=embedding,\n",
        "        seed=seed,\n",
        "        flow_prompt_token=flow_prompt_token,\n",
        "        speech_feat=speech_feat,\n",
        "        device=DEVICE,\n",
        "        use_phoneme=use_phoneme,\n",
        "    )\n",
        "\n",
        "    return tts_speech\n",
        "\n",
        "\n",
        "# Run synthesis\n",
        "tts_speech = glmtts_synthesize(\n",
        "    prompt_wav=prompt_wav_path,\n",
        "    prompt_text=prompt_text,\n",
        "    synth_text=input_text,\n",
        "    sample_rate=sample_rate,\n",
        "    use_phoneme=use_phoneme,\n",
        "    seed=seed,\n",
        "    use_cache=use_cache,\n",
        ")"
      ],
      "metadata": {
        "id": "nG5R1NMhIbi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play audio directly from tensor\n",
        "waveform = tts_speech.squeeze().cpu().numpy()\n",
        "display(Audio(waveform, rate=sample_rate))"
      ],
      "metadata": {
        "id": "O2ePorQEIcmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voice Clone"
      ],
      "metadata": {
        "id": "TwhMkSCjD-yB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup: Clone repo, install deps, download checkpoints\n",
        "\n",
        "!git clone https://github.com/zai-org/GLM-TTS.git\n",
        "%cd /content/GLM-TTS\n",
        "\n",
        "# Remove any line containing WeTextProcessing from requirements.txt\n",
        "import re\n",
        "\n",
        "req_path = \"requirements.txt\"\n",
        "\n",
        "with open(req_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Filter out the problematic line(s)\n",
        "cleaned = []\n",
        "for line in lines:\n",
        "    if not re.search(r'wetextprocessing', line, re.IGNORECASE):\n",
        "        cleaned.append(line)\n",
        "\n",
        "with open(req_path, \"w\") as f:\n",
        "    f.writelines(cleaned)"
      ],
      "metadata": {
        "id": "Beb6PtAJEABH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âš ï¸ You may need to restart session after installing these libraries, then run following cells\n",
        "!pip install -r requirements.txt\n",
        "!pip install -U \"huggingface-hub>=0.34.0,<1.0\"\n",
        "!pip install WeTextProcessing"
      ],
      "metadata": {
        "id": "lncHJDFr4qSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# Download full GLM-TTS checkpoint into ./ckpt\n",
        "snapshot_download(\n",
        "    \"zai-org/GLM-TTS\",\n",
        "    local_dir=\"ckpt\",\n",
        "    local_dir_use_symlinks=False,\n",
        ")"
      ],
      "metadata": {
        "id": "yHYZSU264s7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference text that matches the built-in reference audio speaker\n",
        "prompt_text = \"In short, we embarked on, a mission to make America great again, for all Americans.\"  #@param {type:\"string\"}\n",
        "\n",
        "# Upload reference audio for cloning\n",
        "prompt_wav_path = \"/content/GLM-TTS/examples/prompt/uploaded.wav\"\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(prompt_wav_path), exist_ok=True)\n",
        "\n",
        "# Prompt user upload\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Validate upload\n",
        "if not uploaded:\n",
        "    raise RuntimeError(\"No file was uploaded.\")\n",
        "\n",
        "# Take the first uploaded file\n",
        "uploaded_filename = next(iter(uploaded))\n",
        "\n",
        "# Basic validation: ensure it's a WAV file\n",
        "if not uploaded_filename.lower().endswith(\".wav\"):\n",
        "    raise ValueError(\"Uploaded file is not a .wav file.\")\n",
        "\n",
        "# Save to desired path\n",
        "with open(prompt_wav_path, \"wb\") as f:\n",
        "    f.write(uploaded[uploaded_filename])\n",
        "\n",
        "print(f\"File saved to: {prompt_wav_path}\")\n"
      ],
      "metadata": {
        "id": "PEe-sqfO5GAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text that GLM-TTS will synthesize\n",
        "input_text = \"How is this voice-cloning quality? Does it sound good?\"  #@param {type:\"string\"}\n",
        "\n",
        "# Inference options\n",
        "sample_rate = 24000  #@param {type:\"integer\"}\n",
        "use_phoneme = False  #@param {type:\"boolean\"}\n",
        "use_cache = True     #@param {type:\"boolean\"}\n",
        "seed = 0             #@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "j9RzWd5l4yqB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/GLM-TTS\n",
        "\n",
        "import torch\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# Import GLM-TTS helpers from the repo\n",
        "from glmtts_inference import load_models, generate_long, DEVICE\n",
        "\n",
        "# Load all frontends + models (LLM + Flow)\n",
        "frontend, text_frontend, speech_tokenizer, llm, flow = load_models(\n",
        "    use_phoneme=use_phoneme,\n",
        "    sample_rate=sample_rate,\n",
        ")\n",
        "\n",
        "def glmtts_synthesize(\n",
        "    prompt_wav: str,\n",
        "    prompt_text: str,\n",
        "    synth_text: str,\n",
        "    sample_rate: int = 24000,\n",
        "    use_phoneme: bool = False,\n",
        "    seed: int = 0,\n",
        "    use_cache: bool = True,\n",
        "):\n",
        "    \"\"\"Minimal wrapper around generate_long for a single utterance.\"\"\"\n",
        "    # Normalize text\n",
        "    prompt_text_norm = text_frontend.text_normalize(prompt_text)\n",
        "    synth_text_norm = text_frontend.text_normalize(synth_text)\n",
        "\n",
        "    # Extract tokens & features from prompt\n",
        "    prompt_text_token = frontend._extract_text_token(prompt_text_norm + \" \")\n",
        "    prompt_speech_token = frontend._extract_speech_token([prompt_wav])\n",
        "    speech_feat = frontend._extract_speech_feat(prompt_wav, sample_rate=sample_rate)\n",
        "    embedding = frontend._extract_spk_embedding(prompt_wav)\n",
        "\n",
        "    cache_speech_token = [prompt_speech_token.squeeze().tolist()]\n",
        "    flow_prompt_token = torch.tensor(cache_speech_token, dtype=torch.int32).to(DEVICE)\n",
        "\n",
        "    # LLM cache (for longer text, can reuse history)\n",
        "    cache = {\n",
        "        \"cache_text\": [prompt_text_norm],\n",
        "        \"cache_text_token\": [prompt_text_token],\n",
        "        \"cache_speech_token\": cache_speech_token,\n",
        "        \"use_cache\": use_cache,\n",
        "    }\n",
        "\n",
        "    uttid = \"simple_demo\"\n",
        "\n",
        "    # Core generation\n",
        "    tts_speech, _, _, _ = generate_long(\n",
        "        frontend=frontend,\n",
        "        text_frontend=text_frontend,\n",
        "        llm=llm,\n",
        "        flow=flow,\n",
        "        text_info=[uttid, synth_text_norm],\n",
        "        cache=cache,\n",
        "        embedding=embedding,\n",
        "        seed=seed,\n",
        "        flow_prompt_token=flow_prompt_token,\n",
        "        speech_feat=speech_feat,\n",
        "        device=DEVICE,\n",
        "        use_phoneme=use_phoneme,\n",
        "    )\n",
        "\n",
        "    return tts_speech"
      ],
      "metadata": {
        "id": "pMVhpdgx41AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run synthesis\n",
        "tts_speech = glmtts_synthesize(\n",
        "    prompt_wav=prompt_wav_path,\n",
        "    prompt_text=prompt_text,\n",
        "    synth_text=input_text,\n",
        "    sample_rate=sample_rate,\n",
        "    use_phoneme=use_phoneme,\n",
        "    seed=seed,\n",
        "    use_cache=use_cache,\n",
        ")"
      ],
      "metadata": {
        "id": "wtQFd-xs-thb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play audio directly from tensor\n",
        "waveform = tts_speech.squeeze().cpu().numpy()\n",
        "display(Audio(waveform, rate=sample_rate))"
      ],
      "metadata": {
        "id": "Pp-Pvv8x42-D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMsvuXPMa1+ZBgvsz+OH7Rj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}