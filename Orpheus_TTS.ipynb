{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Troyanovsky/awesome-TTS-Colab/blob/main/Orpheus_TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B51gJgKjOsd"
      },
      "source": [
        "# üó£Ô∏è Orpheus TTS Colab\n",
        "\n",
        "## üìÑ Description\n",
        "\n",
        "This Colab notebook runs **Orpheus TTS**, an **open-source, LLM-based text-to-speech system** built on a **LLaMA-3B backbone**.\n",
        "Orpheus showcases the **emergent capabilities of large language models for speech synthesis**, delivering **highly expressive**, **human-like**, and **low-latency** voice generation with **zero-shot voice cloning**.\n",
        "\n",
        "**Capabilities:**\n",
        "Human-Like Expressive Speech, Zero-Shot Voice Cloning, Guided Emotion & Intonation Tags, Low-Latency Streaming (~200ms), Realtime-Ready TTS\n",
        "\n",
        "---\n",
        "\n",
        "## How to use\n",
        "\n",
        "* Run the first cell to set up dependencies\n",
        "* Modify the input text for speech generation\n",
        "* Run all remaining cells to generate audio output\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Model Highlights\n",
        "\n",
        "* üó£ **State-of-the-art expressiveness** ‚Äì natural rhythm, emotion, and intonation rivaling closed-source models\n",
        "* üé≠ **Emotion & intonation control** ‚Äì guide speech style with simple tags (e.g., `<laugh>`, `<sigh>`)\n",
        "* üß¨ **Zero-shot voice cloning** ‚Äì no fine-tuning required\n",
        "* ‚ö° **Low-latency streaming** ‚Äì ~200ms latency, reducible to ~100ms with input streaming\n",
        "* üåç **Research-backed scalability** ‚Äì part of a larger multilingual model family\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Model Details\n",
        "\n",
        "* **Base Model:** LLaMA-3B\n",
        "* **Model Variant:** Finetuned Prod (English)\n",
        "* **Training Data:** 100k+ hours of English speech (pretraining)\n",
        "* **Supported Language:** English\n",
        "* **Voices:** Multiple preset speaker options (e.g., tara, leah, jess, leo, dan, mia, zac, zoe)\n",
        "* **Use Case:** Production-ready, real-time expressive TTS\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Resources\n",
        "\n",
        "* **GitHub Repository:** https://github.com/canopyai/Orpheus-TTS\n",
        "* **Model Card:** https://huggingface.co/canopylabs/orpheus-3b-0.1-ft\n",
        "\n",
        "---\n",
        "\n",
        "## üéôÔ∏è Explore More TTS Models\n",
        "\n",
        "Looking for more cutting-edge voice models?\n",
        "üëâ Check out the full collection: [awesome-TTS-Colab](https://github.com/Troyanovsky/awesome-TTS-Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voice Generation"
      ],
      "metadata": {
        "id": "wvRGyphHRWCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with huggingface for model access\n",
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "Fni_zeWHSqPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q snac transformers soundfile librosa ipywebrtc"
      ],
      "metadata": {
        "id": "ZtdIpkJyRYF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from snac import SNAC\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "\n",
        "model_name = \"canopylabs/orpheus-3b-0.1-ft\"\n",
        "\n",
        "# Load SNAC codec model (24kHz)\n",
        "snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\")\n",
        "snac_model = snac_model.to(\"cpu\")  # keep codec on CPU\n",
        "\n",
        "# Load Orpheus TTS (LLM backbone)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32)\n",
        "model = model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Loaded Orpheus TTS on {device} and SNAC codec on CPU.\")"
      ],
      "metadata": {
        "id": "caipXakSRY8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Available English voices (from docs): \"tara\", \"leah\", \"jess\", \"leo\", \"dan\", \"mia\", \"zac\", \"zoe\"\n",
        "voice_name = \"tara\"  # üëà change to another voice if you like\n",
        "\n",
        "prompt_text = (\n",
        "    \"Hey there, I'm Orpheus, a text-to-speech model that can speak with natural rhythm and emotion. \"\n",
        "    \"<chuckle> It's nice to meet you in this Colab notebook!\"\n",
        ")  # üëà change this to whatever you want the model to say\n",
        "\n",
        "# Format prompt as expected by Orpheus finetuned model\n",
        "full_prompt = f\"{voice_name}: {prompt_text}\"\n",
        "print(\"Using prompt:\\n\", full_prompt)"
      ],
      "metadata": {
        "id": "yhGDa8-jRi4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Special tokens (from Orpheus example)\n",
        "start_token = torch.tensor([[128259]], dtype=torch.int64)        # Start of human (SOH)\n",
        "end_tokens = torch.tensor([[128009, 128260]], dtype=torch.int64) # End of text (EOT), End of human (EOH)\n",
        "\n",
        "# Tokenize text\n",
        "input_ids = tokenizer(full_prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# SOH SOT Text EOT EOH\n",
        "modified_input_ids = torch.cat([start_token, input_ids, end_tokens], dim=1)\n",
        "\n",
        "# Move to device\n",
        "input_ids = modified_input_ids.to(device)\n",
        "attention_mask = torch.ones_like(input_ids, dtype=torch.int64).to(device)\n",
        "\n",
        "# Generate audio\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=1200,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.1,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=128258,  # end of codes\n",
        "    )\n",
        "\n",
        "# Parse output\n",
        "token_to_find = 128257  # code start token\n",
        "token_to_remove = 128258  # code end token\n",
        "\n",
        "token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)\n",
        "\n",
        "if len(token_indices[1]) > 0:\n",
        "    last_occurrence_idx = token_indices[1][-1].item()\n",
        "    cropped_tensor = generated_ids[:, last_occurrence_idx + 1 :]\n",
        "else:\n",
        "    cropped_tensor = generated_ids\n",
        "\n",
        "# Remove the end token\n",
        "mask = cropped_tensor != token_to_remove\n",
        "\n",
        "processed_rows = []\n",
        "for row in cropped_tensor:\n",
        "    masked_row = row[row != token_to_remove]\n",
        "    processed_rows.append(masked_row)\n",
        "\n",
        "code_lists = []\n",
        "for row in processed_rows:\n",
        "    row_length = row.size(0)\n",
        "    new_length = (row_length // 7) * 7\n",
        "    trimmed_row = row[:new_length]\n",
        "    trimmed_row = [t - 128266 for t in trimmed_row]\n",
        "    code_lists.append(trimmed_row)\n",
        "\n",
        "def redistribute_codes(code_list):\n",
        "    layer_1 = []\n",
        "    layer_2 = []\n",
        "    layer_3 = []\n",
        "    for i in range((len(code_list) + 1) // 7):\n",
        "        layer_1.append(code_list[7 * i])\n",
        "        layer_2.append(code_list[7 * i + 1] - 4096)\n",
        "        layer_3.append(code_list[7 * i + 2] - (2 * 4096))\n",
        "        layer_3.append(code_list[7 * i + 3] - (3 * 4096))\n",
        "        layer_2.append(code_list[7 * i + 4] - (4 * 4096))\n",
        "        layer_3.append(code_list[7 * i + 5] - (5 * 4096))\n",
        "        layer_3.append(code_list[7 * i + 6] - (6 * 4096))\n",
        "\n",
        "    codes = [\n",
        "        torch.tensor(layer_1).unsqueeze(0),\n",
        "        torch.tensor(layer_2).unsqueeze(0),\n",
        "        torch.tensor(layer_3).unsqueeze(0),\n",
        "    ]\n",
        "    audio_hat = snac_model.decode(codes)  # [1, T]\n",
        "    return audio_hat\n",
        "\n",
        "my_samples = []\n",
        "for code_list in code_lists:\n",
        "    samples = redistribute_codes(code_list)\n",
        "    my_samples.append(samples)\n",
        "\n",
        "# Fix: Add .detach() before .numpy() to prevent RuntimeError for tensors requiring grad\n",
        "audio_tensor = my_samples[0].cpu().detach().numpy().squeeze()\n",
        "\n",
        "# SNAC 24kHz model, so use 24000 Hz\n",
        "sample_rate = 24000"
      ],
      "metadata": {
        "id": "Zs51UrDCRoYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Playing generated audio...\")\n",
        "ipd.display(ipd.Audio(audio_tensor, rate=sample_rate))"
      ],
      "metadata": {
        "id": "8HdYrqxQRwLG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNzqCqr4p9cSH4qlUdxIRon",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}