{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Troyanovsky/awesome-TTS-Colab/blob/main/VibeVoice%201.5B%20TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B51gJgKjOsd"
      },
      "source": [
        "# ğŸ—£ï¸ VibeVoice TTS Microsoft Colab\n",
        "\n",
        "## ğŸ“„ Description  \n",
        "This Colab notebook uses VibeVoice TTS to generate expressive, long-form, multi-speaker conversational audio, such as podcasts, from text.\n",
        "\n",
        "**Capabilities**: Context-Aware Expression, Multi-lingual conversation, Podcast with Background Music, Long Conversational Speech\n",
        "\n",
        "---\n",
        "\n",
        "## How to use\n",
        "\n",
        "- Follow the instructions from the comments to change the script_text\n",
        "- Run all cells in the section you need\n",
        "- The generated output will be in `output.wav`\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”— Resources\n",
        "\n",
        "- **GitHub Repository:** https://github.com/microsoft/VibeVoice\n",
        "- **Model Availability:** https://huggingface.co/microsoft/VibeVoice-1.5B\n",
        "\n",
        "---\n",
        "\n",
        "## Special note\n",
        "\n",
        "- English and Chinese only.\n",
        "- VibeVoice is a novel framework designed for generating expressive, long-form, multi-speaker conversational audio.\n",
        "- A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz.\n",
        "- The model can synthesize speech up to 90 minutes long with up to 4 distinct speakers, surpassing the typical 1-2 speaker limits of many prior models.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ™ï¸ Explore More TTS Models  \n",
        "Want to try out additional TTS models? Check out the curated collection here:  \n",
        "ğŸ‘‰ [awesome-TTS-Colab](https://github.com/Troyanovsky/awesome-TTS-Colab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install dependencies and clone the repository\n",
        "print(\"â³ Installing dependencies and setting up the environment...\")\n",
        "# Install ffmpeg for audio processing\n",
        "!apt-get update -y -qq > /dev/null\n",
        "!apt-get install -y ffmpeg -qq > /dev/null\n",
        "\n",
        "# Clone the VibeVoice repository if it doesn't exist\n",
        "import os\n",
        "if not os.path.exists('VibeVoice'):\n",
        "  !git clone https://github.com/microsoft/VibeVoice.git > /dev/null\n",
        "%cd VibeVoice\n",
        "\n",
        "# Install flash-attn and the VibeVoice package\n",
        "!pip install flash-attn --no-build-isolation -qq\n",
        "!pip install -e . -qq\n",
        "\n",
        "print(\"âœ… Environment setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcs6jw12RJ5W",
        "outputId": "e7ef483a-5a81-40b7-abcb-5beadf06eb48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Installing dependencies and setting up the environment...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Cloning into 'VibeVoice'...\n",
            "remote: Enumerating objects: 224, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 224 (delta 43), reused 41 (delta 20), pack-reused 149 (from 1)\u001b[K\n",
            "Receiving objects: 100% (224/224), 85.72 MiB | 31.79 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n",
            "/content/VibeVoice\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.5/35.5 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for vibevoice (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 45.0.6 which is incompatible.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… Environment setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parameters ---\n",
        "# ğŸ“ Paste your multi-speaker script below:\n",
        "script_text = \"\"\"\n",
        "Speaker 1: The hype was immense, with teasers and leaks building for weeks.\n",
        "Speaker 2: Great to be here, Alice. It's certainly been an eventful launch.\n",
        "Speaker 1: And we also have Frank, a tech enthusiast and a super-user.\n",
        "Speaker 3: Hey, Alice. Happy to be here.\n",
        "\"\"\"\n",
        "\n",
        "# ğŸ—£ï¸ Map Speakers to Available Voices\n",
        "# Use a Python dictionary format. The keys should exactly match the speaker names in your script.\n",
        "# Available voices: `Alice`, `Carter`, `Frank`, `Maya`, `Sanuel`, `Anchen`, `Bowen`, `Xinran`\n",
        "speaker_voice_mapping = \"{'Speaker 1': 'Alice', 'Speaker 2': 'Carter', 'Speaker 3': 'Frank'}\""
      ],
      "metadata": {
        "id": "-Dk8CCukgw-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import time\n",
        "import ast\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# --- Import VibeVoice components ---\n",
        "from vibevoice.modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference\n",
        "from vibevoice.processor.vibevoice_processor import VibeVoiceProcessor\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.set_verbosity_info()\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "# --- Helper Class from the original script (with the fix) ---\n",
        "class VoiceMapper:\n",
        "    def __init__(self):\n",
        "        self.setup_voice_presets()\n",
        "        new_dict = {}\n",
        "        for name, path in self.voice_presets.items():\n",
        "            if '_' in name: name = name.split('_')[0]\n",
        "            if '-' in name: name = name.split('-')[-1]\n",
        "            new_dict[name] = path\n",
        "        self.voice_presets.update(new_dict)\n",
        "\n",
        "    def setup_voice_presets(self):\n",
        "        # FIX: Corrected the path to the voices directory to be a simple relative path.\n",
        "        voices_dir = \"demo/voices\"\n",
        "        if not os.path.isdir(voices_dir):\n",
        "            self.voice_presets, self.available_voices = {}, {}\n",
        "            print(f\"âŒ Error: Voices directory not found at path: {os.path.abspath(voices_dir)}\")\n",
        "            return\n",
        "\n",
        "        wav_files = [f for f in os.listdir(voices_dir) if f.lower().endswith('.wav')]\n",
        "        self.voice_presets = {os.path.splitext(f)[0]: os.path.join(voices_dir, f) for f in wav_files}\n",
        "        self.voice_presets = dict(sorted(self.voice_presets.items()))\n",
        "        self.available_voices = {n: p for n, p in self.voice_presets.items() if os.path.exists(p)}\n",
        "        print(f\"âœ… Found {len(self.available_voices)} voice files. Available voices: {', '.join(self.available_voices.keys())}\")\n",
        "\n",
        "    def get_voice_path(self, speaker_name: str) -> str:\n",
        "        speaker_lower = speaker_name.lower()\n",
        "        for preset_name, path in self.voice_presets.items():\n",
        "            if preset_name.lower() == speaker_lower: return path\n",
        "        for preset_name, path in self.voice_presets.items():\n",
        "            if speaker_lower in preset_name.lower(): return path\n",
        "        default_voice = list(self.voice_presets.values())[0]\n",
        "        print(f\"âš ï¸ Warning: No voice preset found for '{speaker_name}', using default voice: {os.path.basename(default_voice)}\")\n",
        "        return default_voice\n",
        "\n",
        "# --- Main Generation Logic ---\n",
        "model_path = \"microsoft/VibeVoice-1.5B\"\n",
        "final_output_file = \"output.wav\"\n",
        "\n",
        "try:\n",
        "    # 1. Parse user inputs\n",
        "    speaker_voice_map = ast.literal_eval(speaker_voice_mapping)\n",
        "    full_script = script_text.strip()\n",
        "\n",
        "    # Automatically detect unique speakers from the script\n",
        "    unique_speakers_in_script = sorted(list(set(re.findall(r\"^(.+?):\", full_script, re.MULTILINE))))\n",
        "    if not unique_speakers_in_script:\n",
        "        raise ValueError(\"No speakers found in the script. Ensure it follows the 'Speaker Name: Text' format.\")\n",
        "    print(f\"âœ… Detected speakers in script: {', '.join(unique_speakers_in_script)}\")\n",
        "\n",
        "    # 2. Map speakers to voice files\n",
        "    voice_mapper = VoiceMapper()\n",
        "    # Add a check to ensure voices were actually found\n",
        "    if not voice_mapper.available_voices:\n",
        "        raise FileNotFoundError(\"Could not find any voice files. Please ensure the 'demo/voices' directory is correct.\")\n",
        "\n",
        "    voice_samples = []\n",
        "    print(\"âœ… Mapping speakers to voices:\")\n",
        "    for speaker in unique_speakers_in_script:\n",
        "        voice_name = speaker_voice_map.get(speaker)\n",
        "        if not voice_name:\n",
        "            raise ValueError(f\"Speaker '{speaker}' from script is not defined in the Speaker-to-Voice mapping.\")\n",
        "        voice_path = voice_mapper.get_voice_path(voice_name)\n",
        "        voice_samples.append(voice_path)\n",
        "        print(f\"  - '{speaker}' -> '{voice_name}' (using {os.path.basename(voice_path)})\")\n",
        "\n",
        "    # 3. Load processor and model\n",
        "    print(\"\\nâ³ Loading processor and model...\")\n",
        "    processor = VibeVoiceProcessor.from_pretrained(model_path)\n",
        "    model = VibeVoiceForConditionalGenerationInference.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map='cuda',\n",
        "        attn_implementation=\"sdpa\" # Use 'sdpa' for T4 GPU compatibility\n",
        "    )\n",
        "    model.eval()\n",
        "    model.set_ddpm_inference_steps(num_steps=10)\n",
        "    print(\"âœ… Model loaded successfully.\")\n",
        "\n",
        "    # 4. Prepare inputs for the model\n",
        "    inputs = processor(\n",
        "        text=[full_script],\n",
        "        voice_samples=[voice_samples],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True,\n",
        "    ).to('cuda')\n",
        "\n",
        "    # 5. Generate audio\n",
        "    print(\"\\nğŸ™ï¸ Generating audio...\")\n",
        "    start_time = time.time()\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=None,\n",
        "        cfg_scale=1.3,\n",
        "        tokenizer=processor.tokenizer,\n",
        "        generation_config={'do_sample': False},\n",
        "        verbose=True,\n",
        "    )\n",
        "    generation_time = time.time() - start_time\n",
        "    print(f\"â±ï¸ Generation finished in {generation_time:.2f} seconds.\")\n",
        "\n",
        "    # 6. Save and display the audio\n",
        "    processor.save_audio(outputs.speech_outputs[0], final_output_file)\n",
        "    print(f\"\\nâœ… Audio successfully generated and saved as '{final_output_file}'.\")\n",
        "    display(Audio(final_output_file, autoplay=True))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "J2MX3TMuRKie"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMorNtp7Cbpzw3elqTlnsNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}